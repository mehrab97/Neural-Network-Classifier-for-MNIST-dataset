{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The images are flattened. We have a 1-D array of images. Instead of the image having 28 rows and 28 columns, we have\n",
    "# one row with 784 pixel values. We neeed to reshape 784 to 28*28 size and pass it through CNN layer 1, followed\n",
    "# by maxpooling layer 1, followed by CNN layer 2, followed by maxpooling layer 2. The O/P obtained is flattened \n",
    "#and passed through the dense layer and then the O/P layer. We implement the same below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing all the constants\n",
    "\n",
    "input_width=28\n",
    "input_height=28\n",
    "input_channels=1\n",
    "input_pixels=784\n",
    "\n",
    "n_conv1=32\n",
    "n_conv2=64\n",
    "stride_conv1=1\n",
    "stride_conv2=1\n",
    "conv1_k=5\n",
    "conv2_k=5\n",
    "max_pool1_k=2\n",
    "max_pool2_k=2\n",
    "\n",
    "n_hidden=1024\n",
    "n_out=10\n",
    "input_size_to_hidden=(input_width//(max_pool1_k*max_pool2_k))*(input_height//(max_pool1_k*max_pool2_k))*n_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the weights and biases\n",
    "\n",
    "#There are no weights in the pooling layer\n",
    "\n",
    "weights={\n",
    "    #CNN layer1 weights would be 5*5*1*32(k*k=5*5, input_channels=1, n_conv1=32)\n",
    "        \"wc1\":tf.Variable(tf.random_normal((conv1_k, conv1_k, input_channels, n_conv1))),\n",
    "    \n",
    "    #CNN layer2 weights would be 5*5*32*64(k*k=5*5, channels of CNN layer2=32, n_conv2=64)\n",
    "        \"wc2\":tf.Variable(tf.random_normal((conv2_k, conv2_k, n_conv1, n_conv2))),\n",
    "    \n",
    "    #Hidden layer weights would be (input height divided by 4) *(input width divided by 4) * 64 * 1024\n",
    "    #(divided by 4 since max pool applied twice) --> done above as input_size_to_hidden\n",
    "        \"wh1\":tf.Variable(tf.random_normal((input_size_to_hidden, n_hidden))),\n",
    "    \n",
    "    #Output weights would be 1024*10\n",
    "        \"wo\":tf.Variable(tf.random_normal((n_hidden, n_out)))}\n",
    "\n",
    "biases={\n",
    "    \"bc1\":tf.Variable(tf.random_normal((n_conv1, ))),\n",
    "    \"bc2\":tf.Variable(tf.random_normal((n_conv2, ))),\n",
    "    \"bh1\":tf.Variable(tf.random_normal((n_hidden, ))),\n",
    "    \"bo\":tf.Variable(tf.random_normal((n_out, )))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation in Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution and maxpool functions\n",
    "\n",
    "#Convolution function.\n",
    "#Incase of convolution function we need to do the convolution i.e. make our filter pass through the image and \n",
    "# create those new values. After that we add the biases. The default function in tensor flow which does the\n",
    "# convolution does not add the biases. So we'll have to add the biases. We can have an activation function too that\n",
    "# can be applied to the resultant of the convolution layer.\n",
    "\n",
    "def conv(x, weights, bias, strides=1):\n",
    "    #Applying filter on the images\n",
    "    #Strides is a list which should be of the same shape as x --> n_images*input_width*input_height*input_channels\n",
    "    #The first rgument is different different images. I'm not going to try and combine 2 images. Want to move 1 by 1.\n",
    "    #So this part is always 1. Also, we won't move along the depth, we're going to exactly fit the filter with the\n",
    "    #image that we have. Same as the number of channels. So both these arguments i.e. n_images & input_channels = 1.\n",
    "    out=tf.nn.conv2d(x, weights, padding='SAME', strides=[1, strides, strides, 1])#k*k form for strides. \n",
    "    #Strides defined above\n",
    "    \n",
    "    #Adding the biases, Special function for adding bias. Allows to add different types of vectors into 1\n",
    "    out=tf.nn.bias_add(out, bias) #could be done using tf.nn.add(out,bias) as well\n",
    "    \n",
    "    #Applying relu on the output\n",
    "    out=tf.nn.relu(out)\n",
    "    return out\n",
    "\n",
    "#Maxpool function\n",
    "def maxpooling(x, k=2): #k signifies what window you want to do the maxpooling on. (2*2 here)\n",
    "    #ksize is the window size--> how to apply maxpooling. Say, I have 100 images, each being 10*10 with 5 channels\n",
    "    # MAxpooling isn't intended to be applied across the images or across the channels. These both are denoted by 1.\n",
    "    # We'll pass strides as well becuase once we've applied window size at one place, how much do we want it to move\n",
    "    # by will be given by strides.\n",
    "    return tf.nn.max_pool(x, padding='SAME', ksize=[1, k, k, 1], strides=[1, k, k, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x, weights, biases): #x is the input\n",
    "    \n",
    "    #Need to reshape our input\n",
    "    #The shape of our input is 784 but we aren't just going to get one image. We'll get multiple images\n",
    "    # and so in case of 100 images, shape would be 100*784\n",
    "    # We want to reshape it to 100*28*28*1 (100*input_width*input_height*input channels)\n",
    "    x=tf.reshape(x, shape=(-1, input_width, input_height, input_channels)) #Element to be reshaped and the desired shape\n",
    "    #When you already know three of the required parameters for the shape, just write -1 for the fourth\n",
    "    #as it will infer it for you\n",
    "    \n",
    "    #Pass it through CNN layer1. conv1 is the output of CNN layer1\n",
    "    conv1=conv(x, weights['wc1'], biases['bc1'], stride_conv1) #Calling the conv function with x, weights and biases of \n",
    "    #CNN layer 1\n",
    "    \n",
    "    #Pass this through maxpool layer1\n",
    "    conv1_pool=maxpooling(conv1, max_pool1_k) #Calling the maxpooling function with output of CNN layer 1 i.e. conv1 \n",
    "    #and pooling size of the maxpool layer 1\n",
    "    \n",
    "    #Pass it through CNN layer 2. conv2 is the output of CNN layer2\n",
    "    conv2=conv(conv1_pool, weights['wc2'], biases['bc2'], stride_conv2) #Calling the conv function with conv1_pool, \n",
    "    #weights and biases of CNN layer 2\n",
    "    \n",
    "    #Pass this through maxpool layer2\n",
    "    conv2_pool=maxpooling(conv2, max_pool2_k) #Calling the maxpooling function with output of CNN layer 2 i.e. conv2\n",
    "    #and pooling size of the maxpool layer 2\n",
    "    \n",
    "    #Input to the hidden layer will come by reshaping the conv2_pool. If conv2_pool was 7*7*64 and 100 such images\n",
    "    # gave 100*7*7*64 then the shape would have been 100*7*7*64\n",
    "    hidden_input=tf.reshape(conv2_pool, shape=(-1, input_size_to_hidden)) #The first element will be inferred by itself\n",
    "    \n",
    "    #We need to pass it through the dense layer\n",
    "    #Multiply weights by input of the hidden layer, add biases and then apply activation function on it\n",
    "    hidden_ouput_before_activation=tf.add(tf.matmul(hidden_input, weights['wh1']), biases['bh1'])\n",
    "    \n",
    "    #Output of the hidden layer\n",
    "    hidden_output=tf.nn.relu(hidden_ouput_before_activation)\n",
    "    \n",
    "    #Not applying any activation function for the output layer\n",
    "    output=tf.add(tf.matmul(hidden_output, weights['wo']), biases['bo'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder('float', shape=(None, input_pixels)) #input\n",
    "y=tf.placeholder(tf.int32, shape=(None, n_out)) #output\n",
    "pred=cnn(x, weights, biases)  #Predictions i.e. output of the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize=optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052515.5018329471\n",
      "29211.698738008738\n",
      "19028.619024150008\n",
      "13545.84865722229\n",
      "14357.581539481878\n",
      "11991.5606413946\n",
      "12426.599194691515\n",
      "10309.33572484029\n",
      "9909.193868816452\n",
      "8321.08828842612\n",
      "7601.565785453555\n",
      "5277.471898220523\n",
      "5830.5974011926255\n",
      "4886.970135342969\n",
      "5420.74832860721\n",
      "4301.238702304959\n",
      "3696.441102922312\n",
      "3930.347788801424\n",
      "3471.715747241911\n",
      "2992.4176112215873\n",
      "2592.2889920090715\n",
      "3019.399624204747\n",
      "3229.108052779778\n",
      "2284.1158180920975\n",
      "2490.208405163884\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "for i in range(25):\n",
    "    num_batches=int(mnist.train.num_examples/batch_size)\n",
    "    total_cost=0\n",
    "    for j in range(num_batches):\n",
    "        batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "        c,_=sess.run([cost,optimize],feed_dict={x:batch_x,y:batch_y})\n",
    "        total_cost+=c\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9840"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=tf.argmax(pred,1)\n",
    "correct_labels=tf.argmax(y,1)\n",
    "correct_predictions=tf.equal(predictions,correct_labels)\n",
    "predictions,correct_preds=sess.run([predictions,correct_predictions],feed_dict={x:mnist.test.images,\n",
    "                                  y:mnist.test.labels})\n",
    "correct_preds.sum() #98.4% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dropout layer to our network\n",
    "#The dropout layer will be added after the hidden layer\n",
    "def cnn(x, weights, biases, keep_prob): #x is the input, keep_prob refers to the probability with which you'll keep \n",
    "#the unit\n",
    "    \n",
    "    #Need to reshape our input\n",
    "    #The shape of our input is 784 but we aren't just going to get one image. We'll get multiple images\n",
    "    # and so in case of 100 images, shape would be 100*784\n",
    "    # We want to reshape it to 100*28*28*1 (100*input_width*input_height*input channels)\n",
    "    x=tf.reshape(x, shape=(-1, input_width, input_height, input_channels)) #Element to be reshaped and the desired \n",
    "    #shape. When you already know three of the required parameters for the shape, just write -1 for the fourth\n",
    "    #as it will infer it for you\n",
    "    \n",
    "    #Pass it through CNN layer1. conv1 is the output of CNN layer1\n",
    "    conv1=conv(x, weights['wc1'], biases['bc1'], stride_conv1) #Calling the conv function with x, weights and biases \n",
    "    #of CNN layer 1\n",
    "    \n",
    "    #Pass this through maxpool layer1\n",
    "    conv1_pool=maxpooling(conv1, max_pool1_k) #Calling the maxpooling function with output of CNN layer 1 i.e. conv1 \n",
    "    #and pooling size of the maxpool layer 1\n",
    "    \n",
    "    #Pass it through CNN layer 2. conv2 is the output of CNN layer2\n",
    "    conv2=conv(conv1_pool, weights['wc2'], biases['bc2'], stride_conv2) #Calling the conv function with conv1_pool, \n",
    "    #weights and biases of CNN layer 2\n",
    "    \n",
    "    #Pass this through maxpool layer2\n",
    "    conv2_pool=maxpooling(conv2, max_pool2_k) #Calling the maxpooling function with output of CNN layer 2 i.e. conv2\n",
    "    #and pooling size of the maxpool layer 2\n",
    "    \n",
    "    #Input to the hidden layer will come by reshaping the conv2_pool. If conv2_pool was 7*7*64 and 100 such images\n",
    "    # gave 100*7*7*64 then the shape would have been 100*7*7*64\n",
    "    hidden_input=tf.reshape(conv2_pool, shape=(-1, input_size_to_hidden)) #The first element will be inferred by \n",
    "    #itself\n",
    "    \n",
    "    #We need to pass it through the dense layer\n",
    "    #Multiply weights by input of the hidden layer, add biases and then apply activation function on it\n",
    "    hidden_ouput_before_activation=tf.add(tf.matmul(hidden_input, weights['wh1']), biases['bh1'])\n",
    "    \n",
    "    #Output of the hidden layer before dropout\n",
    "    hidden_output_before_dropout=tf.nn.relu(hidden_ouput_before_activation)\n",
    "    \n",
    "    #Passing it through the dropout layer\n",
    "    hidden_output=tf.nn.dropout(hidden_output_before_dropout,keep_prob)\n",
    "    \n",
    "    #Not applying any activation function for the output layer\n",
    "    output=tf.add(tf.matmul(hidden_output, weights['wo']), biases['bo'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-4ff2fddb23b6>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder('float', shape=(None, input_pixels)) #input\n",
    "y=tf.placeholder(tf.int32, shape=(None, n_out)) #output\n",
    "keep_prob=tf.placeholder('float')\n",
    "pred=cnn(x, weights, biases, keep_prob)  #Predictions i.e. output of the forward propagation, keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize=optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872747.0000053644\n",
      "34249.23302859068\n",
      "20253.851375933737\n",
      "13011.363422049477\n",
      "10665.460250850952\n",
      "8568.314923640064\n",
      "7578.996290233345\n",
      "6405.353697190732\n",
      "6667.005616922814\n",
      "4443.934203997217\n",
      "5317.131280366819\n",
      "4572.758816115221\n",
      "4384.672963370725\n",
      "4431.369115413308\n",
      "3354.9754682826683\n",
      "3906.374495007949\n",
      "2560.3539869821607\n",
      "3085.9683850681154\n",
      "3019.7926850739955\n",
      "2487.814646050474\n",
      "2449.850273968462\n",
      "2395.42541110889\n",
      "1976.1401373692206\n",
      "1934.5229387753607\n",
      "1876.2988920589673\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "for i in range(25):\n",
    "    num_batches=int(mnist.train.num_examples/batch_size)\n",
    "    total_cost=0\n",
    "    for j in range(num_batches):\n",
    "        batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "        c,_=sess.run([cost,optimize],feed_dict={x:batch_x,y:batch_y, keep_prob : 0.8}) #Training,\n",
    "        #If you want to have a dropout layer, pass a non-1 probability here. keep_prob=1 implies not using any\n",
    "        #dropout layer i.e. keeping all the units.\n",
    "        total_cost+=c\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=tf.argmax(pred,1)\n",
    "correct_labels=tf.argmax(y,1)\n",
    "correct_predictions=tf.equal(predictions,correct_labels)\n",
    "predictions,correct_preds=sess.run([predictions,correct_predictions],feed_dict={x:mnist.test.images, #testing\n",
    "                                  y:mnist.test.labels, keep_prob : 1.0})\n",
    "correct_preds.sum() # 98.13% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
